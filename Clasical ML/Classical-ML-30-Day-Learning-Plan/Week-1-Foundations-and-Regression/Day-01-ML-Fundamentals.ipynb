{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Machine Learning Fundamentals\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "- Understand different types of machine learning\n",
    "- Learn ML algorithm categories\n",
    "- Distinguish between classification, regression, and clustering\n",
    "- Understand parametric vs non-parametric algorithms\n",
    "- Grasp linear vs nonlinear algorithms\n",
    "\n",
    "## ðŸ“š Table of Contents\n",
    "1. [Introduction to Machine Learning](#1-introduction)\n",
    "2. [Types of Machine Learning](#2-types)\n",
    "3. [ML Algorithm Categories](#3-categories)\n",
    "4. [Problem Types](#4-problems)\n",
    "5. [Algorithm Characteristics](#5-characteristics)\n",
    "6. [Hands-on Examples](#6-examples)\n",
    "7. [Exercises](#7-exercises)\n",
    "8. [Summary](#8-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_regression, make_blobs, load_iris, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Introduction to Machine Learning <a id='1-introduction'></a>\n",
    "\n",
    "**Machine Learning** is a subset of Artificial Intelligence that enables systems to learn and improve from experience without being explicitly programmed.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Learning**: Improving performance on a task through experience\n",
    "- **Data**: The experience from which we learn\n",
    "- **Model**: The learned representation of patterns in data\n",
    "- **Prediction**: Using the model to make decisions on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Types of Machine Learning <a id='2-types'></a>\n",
    "\n",
    "There are three main types of machine learning based on the nature of the learning signal or feedback available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Supervised Learning\n",
    "\n",
    "Learning from **labeled data** where we have input-output pairs.\n",
    "\n",
    "**Goal**: Learn a mapping function from inputs (X) to outputs (y)\n",
    "\n",
    "**Formula**: $y = f(X)$\n",
    "\n",
    "**Examples**:\n",
    "- Email spam detection (spam/not spam)\n",
    "- House price prediction\n",
    "- Image classification\n",
    "- Medical diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a supervised learning dataset (classification)\n",
    "X_supervised, y_supervised = make_classification(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Visualize supervised learning data\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_supervised[:, 0], X_supervised[:, 1], \n",
    "                     c=y_supervised, cmap='viridis', \n",
    "                     alpha=0.6, edgecolors='black', s=50)\n",
    "plt.colorbar(scatter, label='Class Label')\n",
    "plt.title('Supervised Learning: Classification Dataset\\n(Each point has a known label)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset shape: {X_supervised.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_supervised))}\")\n",
    "print(f\"Class distribution: {np.bincount(y_supervised)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Unsupervised Learning\n",
    "\n",
    "Learning from **unlabeled data** to find hidden patterns or structures.\n",
    "\n",
    "**Goal**: Discover the underlying structure in data\n",
    "\n",
    "**Examples**:\n",
    "- Customer segmentation\n",
    "- Anomaly detection\n",
    "- Dimensionality reduction\n",
    "- Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an unsupervised learning dataset\n",
    "X_unsupervised, y_true = make_blobs(\n",
    "    n_samples=300,\n",
    "    centers=3,\n",
    "    n_features=2,\n",
    "    cluster_std=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Visualize unsupervised learning data (without labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_unsupervised[:, 0], X_unsupervised[:, 1], \n",
    "           alpha=0.6, edgecolors='black', s=50, c='gray')\n",
    "plt.title('Unsupervised Learning: Unlabeled Dataset\\n(No labels available - find patterns!)', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset shape: {X_unsupervised.shape}\")\n",
    "print(f\"No labels available - algorithm must find structure!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Semi-Supervised Learning\n",
    "\n",
    "Learning from a **mix of labeled and unlabeled data**.\n",
    "\n",
    "**Goal**: Leverage both labeled and unlabeled data for better performance\n",
    "\n",
    "**Use Case**: When labeling is expensive or time-consuming\n",
    "\n",
    "**Examples**:\n",
    "- Image classification with limited labels\n",
    "- Speech recognition\n",
    "- Web content classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create semi-supervised dataset (some labeled, most unlabeled)\n",
    "n_labeled = 50  # Only 50 out of 300 samples are labeled\n",
    "X_semi = X_supervised.copy()\n",
    "y_semi = y_supervised.copy()\n",
    "y_semi[n_labeled:] = -1  # -1 indicates unlabeled data\n",
    "\n",
    "# Visualize semi-supervised learning data\n",
    "plt.figure(figsize=(8, 6))\n",
    "labeled_mask = y_semi != -1\n",
    "unlabeled_mask = y_semi == -1\n",
    "\n",
    "# Plot labeled data\n",
    "scatter_labeled = plt.scatter(X_semi[labeled_mask, 0], X_semi[labeled_mask, 1],\n",
    "                             c=y_semi[labeled_mask], cmap='viridis',\n",
    "                             alpha=0.8, edgecolors='black', s=100,\n",
    "                             label=f'Labeled ({n_labeled} samples)', marker='o')\n",
    "\n",
    "# Plot unlabeled data\n",
    "plt.scatter(X_semi[unlabeled_mask, 0], X_semi[unlabeled_mask, 1],\n",
    "           c='lightgray', alpha=0.4, edgecolors='gray', s=50,\n",
    "           label=f'Unlabeled ({sum(unlabeled_mask)} samples)', marker='x')\n",
    "\n",
    "plt.colorbar(scatter_labeled, label='Class Label (for labeled data)')\n",
    "plt.title('Semi-Supervised Learning: Mix of Labeled and Unlabeled Data', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total samples: {len(y_semi)}\")\n",
    "print(f\"Labeled samples: {sum(labeled_mask)} ({sum(labeled_mask)/len(y_semi)*100:.1f}%)\")\n",
    "print(f\"Unlabeled samples: {sum(unlabeled_mask)} ({sum(unlabeled_mask)/len(y_semi)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Comparison of Learning Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Supervised\n",
    "axes[0].scatter(X_supervised[:, 0], X_supervised[:, 1], \n",
    "               c=y_supervised, cmap='viridis', alpha=0.6, edgecolors='black', s=50)\n",
    "axes[0].set_title('Supervised Learning\\n(All data labeled)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Unsupervised\n",
    "axes[1].scatter(X_unsupervised[:, 0], X_unsupervised[:, 1], \n",
    "               alpha=0.6, edgecolors='black', s=50, c='gray')\n",
    "axes[1].set_title('Unsupervised Learning\\n(No labels)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Semi-supervised\n",
    "axes[2].scatter(X_semi[labeled_mask, 0], X_semi[labeled_mask, 1],\n",
    "               c=y_semi[labeled_mask], cmap='viridis', alpha=0.8, \n",
    "               edgecolors='black', s=100, marker='o')\n",
    "axes[2].scatter(X_semi[unlabeled_mask, 0], X_semi[unlabeled_mask, 1],\n",
    "               c='lightgray', alpha=0.4, edgecolors='gray', s=50, marker='x')\n",
    "axes[2].set_title('Semi-Supervised Learning\\n(Partial labels)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "axes[2].set_ylabel('Feature 2')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ML Algorithm Categories <a id='3-categories'></a>\n",
    "\n",
    "Machine learning problems can be categorized into three main types based on the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Classification\n",
    "\n",
    "**Goal**: Predict discrete categories or classes\n",
    "\n",
    "**Output**: Categorical (e.g., \"spam\" or \"not spam\")\n",
    "\n",
    "**Types**:\n",
    "- **Binary Classification**: 2 classes (Yes/No, True/False)\n",
    "- **Multiclass Classification**: 3+ classes (Dog/Cat/Bird)\n",
    "- **Multilabel Classification**: Multiple labels per sample\n",
    "\n",
    "**Common Algorithms**: Logistic Regression, Decision Trees, SVM, Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Example: Iris Dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train classifier\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLASSIFICATION EXAMPLE: Iris Species Prediction\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset: {iris.data.shape[0]} samples, {iris.data.shape[1]} features\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"\\nâœ… Classification Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Regression\n",
    "\n",
    "**Goal**: Predict continuous numerical values\n",
    "\n",
    "**Output**: Continuous (e.g., price, temperature, age)\n",
    "\n",
    "**Examples**:\n",
    "- House price prediction\n",
    "- Stock price forecasting\n",
    "- Temperature prediction\n",
    "- Sales forecasting\n",
    "\n",
    "**Common Algorithms**: Linear Regression, Polynomial Regression, Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Example\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate regression dataset\n",
    "X_reg, y_reg = make_regression(n_samples=100, n_features=1, noise=15, random_state=42)\n",
    "\n",
    "# Train model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_reg, y_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_reg = reg.predict(X_reg)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_reg, y_pred_reg)\n",
    "r2 = r2_score(y_reg, y_pred_reg)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_reg, y_reg, alpha=0.6, s=50, edgecolors='black', label='Actual Data')\n",
    "plt.plot(X_reg, y_pred_reg, color='red', linewidth=2, label='Regression Line')\n",
    "plt.title('Regression: Predicting Continuous Values', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Feature (X)', fontsize=12)\n",
    "plt.ylabel('Target (y)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"REGRESSION EXAMPLE: Continuous Value Prediction\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMean Squared Error: {mse:.2f}\")\n",
    "print(f\"RÂ² Score: {r2:.3f}\")\n",
    "print(f\"\\nModel equation: y = {reg.coef_[0]:.2f}x + {reg.intercept_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Clustering\n",
    "\n",
    "**Goal**: Group similar data points together\n",
    "\n",
    "**Output**: Cluster assignments (group IDs)\n",
    "\n",
    "**Examples**:\n",
    "- Customer segmentation\n",
    "- Document organization\n",
    "- Image segmentation\n",
    "- Anomaly detection\n",
    "\n",
    "**Common Algorithms**: K-Means, Hierarchical Clustering, DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering Example\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Use the unsupervised dataset\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_unsupervised)\n",
    "\n",
    "# Visualize clustering results\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_unsupervised[:, 0], X_unsupervised[:, 1], \n",
    "                     c=clusters, cmap='viridis', \n",
    "                     alpha=0.6, s=50, edgecolors='black')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "           marker='X', s=300, c='red', edgecolors='black', linewidths=2,\n",
    "           label='Cluster Centers')\n",
    "plt.colorbar(scatter, label='Cluster ID')\n",
    "plt.title('Clustering: Grouping Similar Data Points', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLUSTERING EXAMPLE: K-Means\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nNumber of clusters: {len(np.unique(clusters))}\")\n",
    "print(f\"Cluster sizes: {np.bincount(clusters)}\")\n",
    "print(f\"Inertia (sum of squared distances): {kmeans.inertia_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Classification\n",
    "axes[0].scatter(iris.data[y_test==0, 0], iris.data[y_test==0, 1], \n",
    "               c='red', alpha=0.6, s=50, label=iris.target_names[0])\n",
    "axes[0].scatter(iris.data[y_test==1, 0], iris.data[y_test==1, 1], \n",
    "               c='blue', alpha=0.6, s=50, label=iris.target_names[1])\n",
    "axes[0].scatter(iris.data[y_test==2, 0], iris.data[y_test==2, 1], \n",
    "               c='green', alpha=0.6, s=50, label=iris.target_names[2])\n",
    "axes[0].set_title('Classification\\n(Discrete Categories)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Sepal Length')\n",
    "axes[0].set_ylabel('Sepal Width')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Regression\n",
    "axes[1].scatter(X_reg, y_reg, alpha=0.6, s=50, edgecolors='black')\n",
    "axes[1].plot(X_reg, y_pred_reg, color='red', linewidth=2)\n",
    "axes[1].set_title('Regression\\n(Continuous Values)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Feature')\n",
    "axes[1].set_ylabel('Target')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Clustering\n",
    "scatter = axes[2].scatter(X_unsupervised[:, 0], X_unsupervised[:, 1],\n",
    "                         c=clusters, cmap='viridis', alpha=0.6, s=50, edgecolors='black')\n",
    "axes[2].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "               marker='X', s=200, c='red', edgecolors='black', linewidths=2)\n",
    "axes[2].set_title('Clustering\\n(Grouping)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "axes[2].set_ylabel('Feature 2')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Algorithm Characteristics <a id='5-characteristics'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Parametric vs Non-Parametric Algorithms\n",
    "\n",
    "#### Parametric Algorithms\n",
    "- **Fixed number of parameters** regardless of training data size\n",
    "- Make strong assumptions about data distribution\n",
    "- **Examples**: Linear Regression, Logistic Regression, Naive Bayes\n",
    "- **Pros**: Fast training, less memory, good for large datasets\n",
    "- **Cons**: May underfit if assumptions are wrong\n",
    "\n",
    "#### Non-Parametric Algorithms\n",
    "- **Number of parameters grows** with training data\n",
    "- Make fewer assumptions about data\n",
    "- **Examples**: KNN, Decision Trees, SVM with RBF kernel\n",
    "- **Pros**: Flexible, can model complex patterns\n",
    "- **Cons**: Slower, more memory, risk of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linear data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.2, X.shape[0])\n",
    "\n",
    "# Parametric Model: Linear Regression (2 parameters: slope and intercept)\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X, y)\n",
    "y_pred_linear = linear_model.predict(X)\n",
    "\n",
    "# Non-Parametric Model: KNN (stores all training data)\n",
    "knn_model = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_model.fit(X, y)\n",
    "y_pred_knn = knn_model.predict(X)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Parametric\n",
    "axes[0].scatter(X, y, alpha=0.5, s=30, label='Actual Data', color='blue')\n",
    "axes[0].plot(X, y_pred_linear, 'r-', linewidth=3, label='Linear Model (Parametric)')\n",
    "axes[0].set_title('Parametric Model: Linear Regression\\n(2 parameters: slope + intercept)', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('X', fontsize=11)\n",
    "axes[0].set_ylabel('y', fontsize=11)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].text(0.5, -0.8, f'Parameters: {linear_model.coef_[0]:.3f}x + {linear_model.intercept_:.3f}',\n",
    "            fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Non-Parametric\n",
    "axes[1].scatter(X, y, alpha=0.5, s=30, label='Actual Data', color='blue')\n",
    "axes[1].plot(X, y_pred_knn, 'g-', linewidth=3, label='KNN Model (Non-Parametric)')\n",
    "axes[1].set_title('Non-Parametric Model: K-Nearest Neighbors\\n(Stores all 100 training points)', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('X', fontsize=11)\n",
    "axes[1].set_ylabel('y', fontsize=11)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].text(0.5, -0.8, f'Stores all {len(X)} training samples',\n",
    "            fontsize=10, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PARAMETRIC vs NON-PARAMETRIC COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nParametric (Linear Regression):\")\n",
    "print(f\"  - Number of parameters: 2 (slope and intercept)\")\n",
    "print(f\"  - Model size: Constant (independent of data size)\")\n",
    "print(f\"  - Training time: Fast\")\n",
    "print(f\"  - Prediction time: Very fast\")\n",
    "print(f\"  - Flexibility: Low (assumes linear relationship)\")\n",
    "\n",
    "print(\"\\nNon-Parametric (KNN):\")\n",
    "print(f\"  - Number of parameters: {len(X)} (stores all training data)\")\n",
    "print(f\"  - Model size: Grows with data\")\n",
    "print(f\"  - Training time: Very fast (just stores data)\")\n",
    "print(f\"  - Prediction time: Slower (computes distances)\")\n",
    "print(f\"  - Flexibility: High (adapts to data patterns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Linear vs Nonlinear Algorithms\n",
    "\n",
    "#### Linear Algorithms\n",
    "- Assume **linear relationship** between features and target\n",
    "- Decision boundary is a straight line (or hyperplane)\n",
    "- **Examples**: Linear Regression, Logistic Regression, Linear SVM\n",
    "- **Best for**: Simple relationships, interpretability\n",
    "\n",
    "#### Nonlinear Algorithms\n",
    "- Can capture **complex, non-linear patterns**\n",
    "- Decision boundary can be curved or irregular\n",
    "- **Examples**: Kernel SVM, Neural Networks, Decision Trees\n",
    "- **Best for**: Complex patterns, high accuracy requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linear classification data (moon-shaped)\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.15, random_state=42)\n",
    "\n",
    "# Linear Classifier\n",
    "linear_svm = SVC(kernel='linear', random_state=42)\n",
    "linear_svm.fit(X_moons, y_moons)\n",
    "linear_accuracy = linear_svm.score(X_moons, y_moons)\n",
    "\n",
    "# Nonlinear Classifier\n",
    "nonlinear_svm = SVC(kernel='rbf', random_state=42)\n",
    "nonlinear_svm.fit(X_moons, y_moons)\n",
    "nonlinear_accuracy = nonlinear_svm.score(X_moons, y_moons)\n",
    "\n",
    "# Create decision boundary visualization\n",
    "def plot_decision_boundary(model, X, y, ax, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "                        edgecolors='black', s=50, alpha=0.7)\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1', fontsize=11)\n",
    "    ax.set_ylabel('Feature 2', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    return scatter\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "plot_decision_boundary(linear_svm, X_moons, y_moons, axes[0],\n",
    "                      f'Linear SVM\\nAccuracy: {linear_accuracy:.2%}')\n",
    "plot_decision_boundary(nonlinear_svm, X_moons, y_moons, axes[1],\n",
    "                      f'Nonlinear SVM (RBF Kernel)\\nAccuracy: {nonlinear_accuracy:.2%}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LINEAR vs NONLINEAR COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nLinear SVM Accuracy: {linear_accuracy:.2%}\")\n",
    "print(f\"Nonlinear SVM Accuracy: {nonlinear_accuracy:.2%}\")\n",
    "print(f\"\\nImprovement with nonlinear model: {(nonlinear_accuracy - linear_accuracy)*100:.1f}%\")\n",
    "print(\"\\nðŸ’¡ Key Insight: Nonlinear models can capture complex patterns that linear models miss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Hands-on Exercises <a id='7-exercises'></a>\n",
    "\n",
    "Now it's your turn to practice! Complete the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Identify the ML Type\n",
    "\n",
    "For each scenario below, identify if it's **supervised**, **unsupervised**, or **semi-supervised** learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Fill in your answers\n",
    "\n",
    "scenarios = {\n",
    "    \"1. Predicting house prices based on historical sales data\": \"supervised\",  # TODO: Your answer\n",
    "    \"2. Grouping customers by purchasing behavior without predefined categories\": \"unsupervised\",  # TODO: Your answer\n",
    "    \"3. Email spam detection with 1000 labeled emails and 100,000 unlabeled\": \"semi-supervised\",  # TODO: Your answer\n",
    "    \"4. Detecting fraudulent credit card transactions with labeled fraud cases\": \"supervised\",  # TODO: Your answer\n",
    "    \"5. Organizing news articles into topics without predefined categories\": \"unsupervised\"  # TODO: Your answer\n",
    "}\n",
    "\n",
    "# Check your answers (uncomment to see solutions)\n",
    "# solutions = {\n",
    "#     \"1\": \"supervised\",\n",
    "#     \"2\": \"unsupervised\",\n",
    "#     \"3\": \"semi-supervised\",\n",
    "#     \"4\": \"supervised\",\n",
    "#     \"5\": \"unsupervised\"\n",
    "# }\n",
    "\n",
    "print(\"Your Answers:\")\n",
    "for scenario, answer in scenarios.items():\n",
    "    print(f\"{scenario}: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Choose the Problem Type\n",
    "\n",
    "Classify these as **classification**, **regression**, or **clustering**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Fill in your answers\n",
    "\n",
    "problems = {\n",
    "    \"1. Predicting tomorrow's stock price\": \"regression\",  # TODO: Your answer\n",
    "    \"2. Detecting if a tumor is malignant or benign\": \"classification\",  # TODO: Your answer\n",
    "    \"3. Segmenting market demographics for targeted advertising\": \"clustering\",  # TODO: Your answer\n",
    "    \"4. Estimating the number of calories in a meal from an image\": \"regression\",  # TODO: Your answer\n",
    "    \"5. Identifying handwritten digits (0-9)\": \"classification\"  # TODO: Your answer\n",
    "}\n",
    "\n",
    "print(\"Your Answers:\")\n",
    "for problem, answer in problems.items():\n",
    "    print(f\"{problem}: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Algorithm Selection\n",
    "\n",
    "Would you use **parametric** or **non-parametric** for these scenarios?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Fill in your answers\n",
    "\n",
    "algorithm_choices = {\n",
    "    \"1. Large dataset (1M+ samples) with clear linear relationship\": \"parametric\",  # TODO\n",
    "    \"2. Small dataset (100 samples) with complex, unknown patterns\": \"non-parametric\",  # TODO\n",
    "    \"3. Real-time prediction system requiring fast inference\": \"parametric\",  # TODO\n",
    "    \"4. Exploratory analysis with no prior assumptions about data\": \"non-parametric\"  # TODO\n",
    "}\n",
    "\n",
    "print(\"Your Answers:\")\n",
    "for scenario, answer in algorithm_choices.items():\n",
    "    print(f\"{scenario}: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Practical Implementation\n",
    "\n",
    "Try implementing a simple classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Complete the code\n",
    "\n",
    "# TODO: Load the wine dataset from sklearn\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# TODO: Split into train and test sets (70-30 split)\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    wine.data, wine.target, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# TODO: Train a Logistic Regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Calculate and print the accuracy\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Model Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Bonus: Try different classifiers and compare results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Summary & Key Takeaways <a id='8-summary'></a>\n",
    "\n",
    "### ðŸŽ¯ What We Learned Today\n",
    "\n",
    "1. **Types of Machine Learning**\n",
    "   - **Supervised**: Learn from labeled data (X â†’ y)\n",
    "   - **Unsupervised**: Find patterns in unlabeled data\n",
    "   - **Semi-supervised**: Mix of labeled and unlabeled data\n",
    "\n",
    "2. **Problem Types**\n",
    "   - **Classification**: Predict discrete categories\n",
    "   - **Regression**: Predict continuous values\n",
    "   - **Clustering**: Group similar data points\n",
    "\n",
    "3. **Algorithm Characteristics**\n",
    "   - **Parametric**: Fixed parameters, fast, assumes data distribution\n",
    "   - **Non-parametric**: Flexible, grows with data, fewer assumptions\n",
    "   - **Linear**: Simple, interpretable, assumes linear relationships\n",
    "   - **Nonlinear**: Complex, accurate, captures non-linear patterns\n",
    "\n",
    "### ðŸ”‘ Key Decision Framework\n",
    "\n",
    "```\n",
    "Do you have labeled data?\n",
    "â”œâ”€ Yes â†’ Supervised Learning\n",
    "â”‚  â”œâ”€ Discrete output? â†’ Classification\n",
    "â”‚  â””â”€ Continuous output? â†’ Regression\n",
    "â”œâ”€ No â†’ Unsupervised Learning\n",
    "â”‚  â””â”€ Group similar items? â†’ Clustering\n",
    "â””â”€ Some labels? â†’ Semi-supervised Learning\n",
    "\n",
    "Is the relationship simple?\n",
    "â”œâ”€ Yes â†’ Linear algorithms\n",
    "â””â”€ No â†’ Nonlinear algorithms\n",
    "\n",
    "Do you have lots of data?\n",
    "â”œâ”€ Yes â†’ Parametric algorithms (faster)\n",
    "â””â”€ No â†’ Non-parametric algorithms (more flexible)\n",
    "```\n",
    "\n",
    "### ðŸ“š Next Steps\n",
    "\n",
    "1. **Tomorrow (Day 2)**: Deep dive into Linear Regression\n",
    "   - Mathematical foundations\n",
    "   - Cost functions and optimization\n",
    "   - Implementation from scratch\n",
    "\n",
    "2. **Practice**:\n",
    "   - Complete all exercises above\n",
    "   - Try different datasets from sklearn\n",
    "   - Experiment with algorithm parameters\n",
    "\n",
    "3. **Additional Reading**:\n",
    "   - [Scikit-learn Documentation](https://scikit-learn.org/stable/)\n",
    "   - [Google ML Crash Course](https://developers.google.com/machine-learning/crash-course)\n",
    "   - \"Hands-On Machine Learning\" - Chapter 1\n",
    "\n",
    "### ðŸ’¡ Pro Tips\n",
    "\n",
    "- Start with simple models before trying complex ones\n",
    "- Always visualize your data first\n",
    "- Understand the problem before choosing an algorithm\n",
    "- Experiment with different approaches\n",
    "- Document your findings and learnings\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! ðŸŽ‰** You've completed Day 1 of your ML journey!\n",
    "\n",
    "**Ready for Day 2?** Continue to [Linear Regression](./Day-02-Linear-Regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Additional Resources\n",
    "\n",
    "### Books\n",
    "- \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" by AurÃ©lien GÃ©ron\n",
    "- \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n",
    "- \"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman\n",
    "\n",
    "### Online Courses\n",
    "- [Andrew Ng's Machine Learning Course](https://www.coursera.org/learn/machine-learning)\n",
    "- [Fast.ai Practical Deep Learning](https://course.fast.ai/)\n",
    "- [Google's Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course)\n",
    "\n",
    "### Practice Platforms\n",
    "- [Kaggle](https://www.kaggle.com/) - Competitions and datasets\n",
    "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)\n",
    "- [OpenML](https://www.openml.org/)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or feedback?** Open an issue in the repository or join our community discussions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}